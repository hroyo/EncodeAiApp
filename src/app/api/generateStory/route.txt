// /app/api/generateStory/route.ts
import { NextResponse } from 'next/server';
import OpenAI from 'openai';
import type { Genre } from '../../system/types'; // Update with the correct path to your types
import { createSystemPrompt, genreList } from '../../system/prompts'; // Update with the correct path to your prompts
import { ChatCompletionMessageParam } from 'openai/resources/chat/completions';

const oai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY, // Use secure server-side environment variables
});

// TypeScript will complain if we don't cast these:
const top_p = Number(process.env.OPENAI_TOP_P || 0.5);
const max_tokens = Number(process.env.OPENAI_MAX_TOKENS || 150);
const temperature = Number(process.env.OPENAI_TEMPERATURE || 0.5);
const presence_penalty = Number(process.env.OPENAI_PRESENCE_PENALTY || 0.5);
const frequency_penalty = Number(process.env.OPENAI_FREQUENCY_PENALTY || 0.5);
const model = process.env.OPENAI_MODEL || 'gpt-3.5-turbo';

async function chatCompletionStream(systemPrompt: string, userPrompt: string) {
    console.log(`Creating chat completion stream with system prompt: ${systemPrompt}`);
    console.log(`User prompt: ${userPrompt}`);
    const messages = [
        {
            role: 'system',
            content: systemPrompt,
        },
        {
            role: 'user',
            content: userPrompt,
        },
    ] as ChatCompletionMessageParam[];

    const response = await oai.chat.completions.create({
        model,
        messages,
        frequency_penalty,
        presence_penalty,
        stream: false, // We aren't using stream here. You can modify this later if needed
        temperature,
        max_tokens,
        top_p,
    });

    console.log('OpenAI Response:', response); // Log the full response from OpenAI

    return response;
}

async function isFlagged(input: string) {
    const response = await oai.moderations.create({ input });
    const [results] = response.results; // We only need the first result

    return results?.flagged === true;
}

async function generateStory(userPrompt: string, genre: Genre) {
    console.log(`OpenAI settings: top_p=${top_p}, max_tokens=${max_tokens}, temperature=${temperature}, presence_penalty=${presence_penalty}, frequency_penalty=${frequency_penalty}`);
    const genrePrompt = genreList[genre];
    const systemPrompt = createSystemPrompt(genrePrompt.prompt, genrePrompt.caveat || '');
    return chatCompletionStream(systemPrompt, userPrompt);
}

// API Route handler
export async function POST(req: Request) {
    try {
        const { userPrompt, genre }: { userPrompt: string; genre: Genre } = await req.json();

        console.log(`Generating story for genre: ${genre}, prompt: ${userPrompt}`);

        // Check if the prompt is flagged
        const flagged = await isFlagged(userPrompt);
        if (flagged) {
            return NextResponse.json({ error: 'Content is flagged as inappropriate.' }, { status: 400 });
        }

        // Generate the story
        const storyResponse = await generateStory(userPrompt, genre);
        console.log('Story response from OpenAI:', storyResponse);

        // Check if storyResponse is valid
        if (!storyResponse || !storyResponse.choices || !storyResponse.choices[0]?.message?.content) {
            console.error('Invalid story response:', storyResponse);
            return NextResponse.json({ error: 'No valid story generated.' }, { status: 500 });
        }

        const story = storyResponse.choices[0].message.content;
        return NextResponse.json({ story });
    } catch (error) {
        console.error('Error in API route:', error);
        return NextResponse.json({ error: 'Failed to generate story.' }, { status: 500 });
    }
}
